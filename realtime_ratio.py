import pandas as pd
import numpy as np
import time
import os
import sys
from collections import deque

# å°è¯•å¯¼å…¥ PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    HAS_TORCH = True
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… PyTorch (pip install torch)")
    HAS_TORCH = False

# ==================== é…ç½®åŒº ====================
CSV_FILENAME = "opc_ratio.csv"
TAG_GLRQLL = "PLC1.xt_apc.glrqll" # è¾“å…¥1: é«˜ç‚‰ç…¤æ°”
TAG_ZLRQLL = "PLC1.xt_apc.zlrqll" # è¾“å…¥2: è½¬ç‚‰ç…¤æ°”
TAG_ZQLL   = "PLC1.xt_apc.zqll"   # ç›®æ ‡: è’¸æ±½äº§é‡

UPDATE_INTERVAL = 1   
BUFFER_CAPACITY = 2000 # å¢å¤§ç¼“å†²åŒºï¼Œçœ‹æ›´è¿œçš„å†å²
TRAIN_BATCH_SIZE = 128 # å¢å¤§ Batchï¼Œè®©æ¢¯åº¦æ›´ç¨³å®š
# å™ªå£°é—¨æ§ï¼šå½“è¾“å…¥å˜åŒ–å¤ªå°æ—¶ï¼Œæ ·æœ¬ä¸å‚ä¸è®­ç»ƒ
# åŸ 0.002 -> é™ä½åˆ° 0.0005ï¼Œæ•æ‰å¾®å°æ³¢åŠ¨
DELTA_GATE_FRAC = 0.0005   
DELTA_GATE_ABS_GL = 0.2    # é™ä½ç»å¯¹é—¨é™
DELTA_GATE_ABS_ZL = 0.2   
# å¹³æ»‘å»å™ª
ENABLE_EMA_SMOOTH = True
EMA_ALPHA = 0.2  # 0.1~0.3 è¶Šå°è¶Šå¹³æ»‘
# ç‰©ç†æ¯”ä¾‹å…ˆéªŒï¼ˆè½¯çº¦æŸï¼‰
ENABLE_RATIO_PRIOR = True
RATIO_TARGET = 1.8
PHYSICS_LOSS_WEIGHT = 50.0 # å¼ºåŠ›çº åï¼šæ—¢ç„¶GLæ³¢åŠ¨å¤šä¸ºå™ªå£°ï¼Œå¿…é¡»ç”¨è¶…å¼ºç‰©ç†çº¦æŸå¯¹æŠ—ç»Ÿè®¡è¡°å‡
# æŒ‰è€å¸ˆæ€è·¯ï¼šä¸¤ç‚¹è§£æ–¹ç¨‹
USE_TWO_POINT_SOLVE = False
TWO_POINT_LOOKBACK = 800
TWO_POINT_STEP = 5
TWO_POINT_GAP = 50
DET_THRESHOLD_FRAC = 1e-4
DELTA_Y_MIN = 2.0
DELTA_X_MIN_FRAC = 0.0015
DELTA_GATE_ABS_Y = 1.0
TWO_POINT_MIN_PAIRS = 20
TWO_POINT_MAX_PAIRS = 400
# æŒ‰è€å¸ˆæ€è·¯ï¼šæ‰¾â€œç‹¬ç«‹å˜åŒ–â€æ®µæ¥è§£
USE_INDEPENDENT_SEGMENTS = True
INDEPENDENT_STABLE_FRAC = 0.6
MIN_INDEPENDENT_SAMPLES = 10
# æ–¹æ¡ˆAï¼šæ»‘åŠ¨çª—å·®åˆ†è¾¨è¯†
USE_SLIDING_DIFF = True
DIFF_WINDOW = 400
MIN_KEEP_DIFF = 60
# æ–¹æ¡ˆBï¼šé˜²å¡Œç¼©ä¸‹é™ï¼ˆä¸æŒ‡å®šæ¯”ä¾‹ï¼Œåªé˜²æ­¢ k_gl å˜æˆ 0ï¼‰
ENABLE_FLOOR_LOSS = True
FLOOR_RATIO_TAU = 0.08
FLOOR_LOSS_WEIGHT = 0.5
# === åˆå§‹ç‰©ç†æ»åçŒœæµ‹ (ç§’) ===
# ç¨åä¼šç”± auto_tune_lag åŠ¨æ€è°ƒæ•´
DEFAULT_LAG_GL = 300
DEFAULT_LAG_ZL = 300
AUTO_TUNE_LAG = True  # å¼€å¯è‡ªåŠ¨æœç´¢ï¼Œè®©æ¨¡å‹è‡ªå·±æ‰¾æœ€ä½³æ»åæ—¶é—´


# LSTM é…ç½®
SEQ_LEN = 15           
HIDDEN_SIZE = 64
LR = 0.005             # æé€Ÿï¼šåŸ0.001 -> 0.005ï¼ŒåŠ å¿«é€‚åº”å¼ºçº¦æŸ
# ================================================

if HAS_TORCH:
    class LSTMTwoInputModel(nn.Module):
        def __init__(self):
            super(LSTMTwoInputModel, self).__init__()
            # input_size=2 (é«˜ç‚‰, è½¬ç‚‰)
            self.lstm = nn.LSTM(input_size=2, hidden_size=HIDDEN_SIZE, num_layers=1, batch_first=True)
            
            # çº¦æŸå±‚: å¼ºåˆ¶æƒé‡ä¸ºæ­£çš„ç‰©ç†çº¦æŸå±‚
            self.fc_energy = nn.Sequential(
                nn.Linear(HIDDEN_SIZE, 16),
                nn.ReLU(),
                nn.Linear(16, 2), # è¾“å‡º [k_gl, k_zl]
                nn.Softplus()     # å¼ºåˆ¶ > 0
            )
            
            # === æ”¹è¿›1: å¼•å…¥å¯å­¦ä¹ çš„ Bias (æˆªè·) ===
            # å¸æ”¶å›ºå®šåº•åº§èƒ½é‡ï¼Œè®© k ä¸“æ³¨äºè§£é‡Šå˜åŒ–é‡
            self.bias = nn.Parameter(torch.zeros(1))

        def forward(self, x):
            # x: (Batch, Seq, 2)
            out, _ = self.lstm(x)
            features = out[:, -1, :] 
            
            # ç³»æ•°é¢„æµ‹
            coeffs = self.fc_energy(features) # (Batch, 2)
            k_gl = coeffs[:, 0:1]
            k_zl = coeffs[:, 1:2]
            
            # ç‰©ç†å…¬å¼: y = k1*x1 + k2*x2 + Bias
            current_inputs = x[:, -1, :]
            gl_in = current_inputs[:, 0:1]
            zl_in = current_inputs[:, 1:2]
            
            pred_y = k_gl * gl_in + k_zl * zl_in + self.bias
            return pred_y, k_gl, k_zl, self.bias
else:
    class LSTMTwoInputModel: pass

class LSTMRatioLearner:
    def __init__(self):
        if not HAS_TORCH: return
        self.device = torch.device("cpu")
        self.model = LSTMTwoInputModel().to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=LR, weight_decay=1e-5) # é™ä½æ­£åˆ™åŒ–ï¼šåŸ1e-3 -> 1e-5, å…è®¸æ›´å¤§çš„ k å€¼
        self.criterion = nn.MSELoss()
        
        # å½’ä¸€åŒ–å‚æ•° (æ”¹ä¸º MaxScalingï¼Œä¿ç•™é›¶ç‚¹ç‰©ç†æ„ä¹‰)
        self.max_in = np.ones(2) 
        self.max_out = 1.0

        self.t = 0
        self.loss_history = deque(maxlen=20)
        
        # ç»“æœå¹³æ»‘å™¨ (EMA)
        self.smooth_ratio = None
        # ç§»é™¤ç¡¬ç¼–ç çŒœæµ‹ï¼Œåˆå§‹åŒ–ä¸ºNoneï¼Œç¬¬ä¸€æ¬¡è®¡ç®—æ—¶ç›´æ¥èµ‹å€¼
        self.smooth_k_gl = None
        self.smooth_k_zl = None
        
        # å…¨å±€ç»Ÿè®¡é”šç‚¹ (åˆå§‹å‡è®¾ ZL çƒ­å€¼çº¦ä¸º GL çš„ 1.8 å€)
        self.base_k_gl_norm = 0.3 
        self.base_k_zl_norm = 0.54
        
        # åŠ¨æ€æ»åå‚æ•° (åˆå§‹åŒ–ä¸ºé»˜è®¤å€¼)
        self.lag_gl = DEFAULT_LAG_GL
        self.lag_zl = DEFAULT_LAG_ZL
        # ç‹¬ç«‹å˜åŒ–æ®µè®¡æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        self.last_indep_gl = 0
        self.last_indep_zl = 0

    def _ema_smooth(self, arr, alpha):
        if not arr:
            return arr
        out = np.zeros(len(arr))
        out[0] = arr[0]
        for i in range(1, len(arr)):
            out[i] = alpha * arr[i] + (1 - alpha) * out[i - 1]
        return out

    def auto_tune_lag(self, gl_arr, zl_arr, zq_arr):
        # === æ”¹è¿›3: å¾®æ­¥æ»åè°ƒæ•´ (Micro-Adjustment) ===
        # æŠ›å¼ƒå…¨å±€æœç´¢ï¼Œæ¯æ¬¡åªå…è®¸å¾®è°ƒ Â±1s
        # é¢‘ç‡æé«˜åˆ°æ¯ 5 æ¬¡è¿­ä»£ä¸€æ¬¡ï¼Œå› ä¸ºæ¯æ¬¡åªåŠ¨ä¸€ç‚¹ç‚¹
        if self.t % 5 != 0: return 
        
        n = len(zq_arr)
        if n < 1000: return 
        
        # çª—å£
        scan_window = 900 
        g_recent = np.array(gl_arr)[-scan_window:]
        z_recent = np.array(zl_arr)[-scan_window:]
        q_recent = np.array(zq_arr)[-scan_window:]
        
        # --- GL å¾®è°ƒ ---
        best_corr_gl = -2 # Correlation range -1 to 1
        best_lag_gl = self.lag_gl
        
        # åªçœ‹ [lag-1, lag, lag+1]
        candidates = [self.lag_gl - 1, self.lag_gl, self.lag_gl + 1]
        
        for lag in candidates:
            if lag < 0: continue
            min_len = min(len(q_recent)-lag, len(g_recent)-lag)
            if min_len < 100: continue
            
            gs = g_recent[:min_len]
            qs = q_recent[lag:lag+min_len]
            
            # ä½¿ç”¨å¢é‡ç›¸å…³æ€§è€Œä¸æ˜¯ç»å¯¹å€¼ç›¸å…³æ€§ï¼
            # å¢é‡ç›¸å…³æ€§å¯¹æ»åæ›´æ•æ„Ÿ
            dgs = np.diff(gs)
            dqs = np.diff(qs)
            
            if np.std(dgs) > 1e-6 and np.std(dqs) > 1e-6:
                c_gl = np.corrcoef(dgs, dqs)[0,1]
                if c_gl > best_corr_gl:
                     best_corr_gl = c_gl
                     best_lag_gl = lag

        # --- ZL å¾®è°ƒ ---
        best_corr_zl = -2 
        best_lag_zl = self.lag_zl
        
        candidates_z = [self.lag_zl - 1, self.lag_zl, self.lag_zl + 1]

        for lag in candidates_z:
            if lag < 0: continue
            min_len = min(len(q_recent)-lag, len(z_recent)-lag)
            if min_len < 100: continue
            
            zs = z_recent[:min_len]
            qs = q_recent[lag:lag+min_len]

            dzs = np.diff(zs)
            dqs = np.diff(qs)

            if np.std(dzs) > 1e-6 and np.std(dqs) > 1e-6:
                c_zl = np.corrcoef(dzs, dqs)[0,1]
                if c_zl > best_corr_zl:
                     best_corr_zl = c_zl
                     best_lag_zl = lag

        # ç›´æ¥æ›´æ–°ï¼Œä¸éœ€è¦å¹³æ»‘ï¼Œå› ä¸ºæ¯æ¬¡åªåŠ¨1ç§’
        self.lag_gl = best_lag_gl
        self.lag_zl = best_lag_zl

    def update_stats(self, gl_data, zl_data, zq_data):
        # 0. å°è¯•è‡ªåŠ¨æ ¡å‡†æ»åæ—¶é—´ (Micro Mode)
        if AUTO_TUNE_LAG:
            self.auto_tune_lag(gl_data, zl_data, zq_data)
    
        # ä½¿ç”¨ç»å¯¹å€¼æœ€å¤§å€¼è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¿ç•™ y=0, x=0 çš„ç‰©ç†åŸç‚¹
        # è¿™æ · y = kx çš„å…¬å¼æ‰æˆç«‹
        if ENABLE_EMA_SMOOTH:
            gl_data = self._ema_smooth(gl_data, EMA_ALPHA)
            zl_data = self._ema_smooth(zl_data, EMA_ALPHA)
            zq_data = self._ema_smooth(zq_data, EMA_ALPHA)

        in_data = np.column_stack((gl_data, zl_data))
        current_max_in = np.max(np.abs(in_data), axis=0)
        current_max_out = np.max(np.abs(zq_data))
        
        # ç¼“æ…¢æ›´æ–°æœ€å¤§å€¼ï¼Œé˜²æ­¢æ³¢åŠ¨å‰§çƒˆ
        self.max_in = np.maximum(self.max_in, current_max_in)
        self.max_out = max(self.max_out, current_max_out)
        
        # 2. è®¡ç®—å…¨å±€çº¿æ€§å›å½’ç³»æ•°
        # å¿…é¡»å¯¹é½æ•°æ®ï¼ï¼ï¼
        max_lag = max(self.lag_gl, self.lag_zl)
        if len(gl_data) > max_lag + 10:
            # ç®€å•çš„å¯¹é½åˆ‡ç‰‡è¿›è¡Œå›å½’
            g_arr = np.array(gl_data)
            z_arr = np.array(zl_data)
            q_arr = np.array(zq_data)
            
            # å¯¹é½: q[t] ~ g[t-lag_gl], z[t-lag_zl]

            limit = len(q_arr)
            Y = q_arr[max_lag : limit]
            
            # åˆ‡ç‰‡æˆªæ­¢ç‚¹
            end_g = limit - self.lag_gl
            if self.lag_gl == 0: end_g = limit
            X_gl = g_arr[max_lag - self.lag_gl : end_g]
            
            end_z = limit - self.lag_zl
            if self.lag_zl == 0: end_z = limit
            X_zl = z_arr[max_lag - self.lag_zl : end_z]
            
            # å†æ¬¡ç¡®è®¤é•¿åº¦ä¸€è‡´
            min_len = min(len(X_gl), len(X_zl), len(Y))
            g_n = X_gl[:min_len] / self.max_in[0]
            z_n = X_zl[:min_len] / self.max_in[1]
            q_n = Y[:min_len]    / self.max_out
            
            try:
                should_update_anchor = False
                if USE_TWO_POINT_SOLVE:
                    # æŒ‰è€å¸ˆæ„è§ï¼šé€‰ä¸¤æ¡æ–¹ç¨‹ç›´æ¥è§£ï¼ˆå¤šå¯¹å–ä¸­ä½æ•°ï¼Œé¿å…åç‚¹ï¼‰
                    thr_gl = max(self.max_in[0] * max(DELTA_GATE_FRAC, DELTA_X_MIN_FRAC), DELTA_GATE_ABS_GL)
                    thr_zl = max(self.max_in[1] * max(DELTA_GATE_FRAC, DELTA_X_MIN_FRAC), DELTA_GATE_ABS_ZL)
                    det_threshold = (self.max_in[0] * self.max_in[1]) * DET_THRESHOLD_FRAC

                    k1_list = []
                    k2_list = []
                    start_idx = max(0, min_len - TWO_POINT_LOOKBACK)
                    for i in range(start_idx, min_len - TWO_POINT_GAP, TWO_POINT_STEP):
                        g1, z1, y1 = X_gl[i], X_zl[i], Y[i]
                        for j in range(i + TWO_POINT_GAP, min_len, TWO_POINT_STEP * 2):
                            g2, z2, y2 = X_gl[j], X_zl[j], Y[j]
                            if abs(g1 - g2) < thr_gl and abs(z1 - z2) < thr_zl:
                                continue
                            if abs(y1 - y2) < DELTA_Y_MIN:
                                continue
                            det = g1 * z2 - g2 * z1
                            if abs(det) < det_threshold:
                                continue
                            k = np.linalg.solve(np.array([[g1, z1], [g2, z2]]), np.array([y1, y2]))
                            k1_raw, k2_raw = k[0], k[1]
                            if np.isfinite(k1_raw) and np.isfinite(k2_raw):
                                k1_list.append(k1_raw)
                                k2_list.append(k2_raw)
                                if len(k1_list) >= TWO_POINT_MAX_PAIRS:
                                    break
                        if len(k1_list) >= TWO_POINT_MAX_PAIRS:
                            break

                    if len(k1_list) < TWO_POINT_MIN_PAIRS:
                        # ä¸å¯è¾¨è¯†ï¼Œä¿æŒåŸé”šç‚¹
                        return

                    # å–ä¸­ä½æ•°ï¼ŒæŠ—å¼‚å¸¸å€¼
                    k1_raw = float(np.median(k1_list))
                    k2_raw = float(np.median(k2_list))
                    k1_safe = max(0.001, k1_raw * (self.max_in[0] / self.max_out))
                    k2_safe = max(0.001, k2_raw * (self.max_in[1] / self.max_out))
                    should_update_anchor = True
                else:
                    # å›é€€ï¼šå·®åˆ†å›å½’ï¼ˆæ»‘åŠ¨çª—ï¼‰
                    if USE_SLIDING_DIFF:
                        start = max(0, min_len - DIFF_WINDOW)
                        Xg_win = X_gl[start:min_len]
                        Xz_win = X_zl[start:min_len]
                        Y_win = Y[start:min_len]
                    else:
                        Xg_win = X_gl[:min_len]
                        Xz_win = X_zl[:min_len]
                        Y_win = Y[:min_len]

                    d_g = np.diff(Xg_win)
                    d_z = np.diff(Xz_win)
                    d_y = np.diff(Y_win)
                    thr_gl = max(self.max_in[0] * DELTA_GATE_FRAC, DELTA_GATE_ABS_GL)
                    thr_zl = max(self.max_in[1] * DELTA_GATE_FRAC, DELTA_GATE_ABS_ZL)
                    thr_y = max(self.max_out * DELTA_GATE_FRAC, DELTA_GATE_ABS_Y, DELTA_Y_MIN)
                    if USE_INDEPENDENT_SEGMENTS:
                        # ç”¨â€œGLåŠ¨ã€ZLç¨³â€çš„æ®µä¼° k_glï¼Œç”¨â€œZLåŠ¨ã€GLç¨³â€çš„æ®µä¼° k_zl
                        stable_z = np.abs(d_z) <= thr_zl * INDEPENDENT_STABLE_FRAC
                        stable_g = np.abs(d_g) <= thr_gl * INDEPENDENT_STABLE_FRAC
                        move_g = (np.abs(d_g) >= thr_gl) & (np.abs(d_y) >= thr_y)
                        move_z = (np.abs(d_z) >= thr_zl) & (np.abs(d_y) >= thr_y)

                        idx_gl = np.where(move_g & stable_z)[0]
                        idx_zl = np.where(move_z & stable_g)[0]
                        self.last_indep_gl = len(idx_gl)
                        self.last_indep_zl = len(idx_zl)

                        if len(idx_gl) >= MIN_INDEPENDENT_SAMPLES and len(idx_zl) >= MIN_INDEPENDENT_SAMPLES:
                            k1_raw = np.median(d_y[idx_gl] / (d_g[idx_gl] + 1e-9))
                            k2_raw = np.median(d_y[idx_zl] / (d_z[idx_zl] + 1e-9))
                            if np.isfinite(k1_raw) and np.isfinite(k2_raw):
                                k1_safe = max(0.001, k1_raw * (self.max_in[0] / self.max_out))
                                k2_safe = max(0.001, k2_raw * (self.max_in[1] / self.max_out))
                                should_update_anchor = True
                            else:
                                return
                        else:
                            return
                    else:
                        keep = ((np.abs(d_g) >= thr_gl) | (np.abs(d_z) >= thr_zl)) & (np.abs(d_y) >= thr_y)
                        if np.sum(keep) < MIN_KEEP_DIFF:
                            return
                        d_gn = d_g[keep] / self.max_in[0]
                        d_zn = d_z[keep] / self.max_in[1]
                        d_yn = d_y[keep] / self.max_out
                        Xd = np.vstack([d_gn, d_zn]).T
                        n_features = Xd.shape[1]
                        lambda_ridge = 0.5
                        I = np.eye(n_features)
                        XTX = Xd.T @ Xd
                        XTy = Xd.T @ d_yn
                        beta = np.linalg.inv(XTX + lambda_ridge * I) @ XTy
                        k1_safe = max(0.001, beta[0])
                        k2_safe = max(0.001, beta[1])
                        y_hat = Xd @ beta
                        mse = np.mean((y_hat - d_yn) ** 2)
                        if np.isfinite(mse) and mse < 0.02 and Xd.shape[0] >= 200:
                            should_update_anchor = True
                        else:
                            return
                
                # === ç‰©ç†ä¿®æ­£ (Physical Correction) ===
                # å¦‚æœç»Ÿè®¡å‡ºæ¥çš„ç³»æ•°æ¯”ä¾‹å¤ªç¦»è°± (ä¾‹å¦‚ k_zl / k_gl > 10)ï¼Œ
                # è¯´æ˜æ•°æ®å…±çº¿æ€§ä¸¥é‡å¯¼è‡´å½’å› é”™è¯¯ã€‚
                # æˆ‘ä»¬å¼ºåˆ¶æŠŠå®ƒä»¬æ‹‰å›åˆ°åˆç†çš„ç‰©ç†æ¯”ä¾‹é™„è¿‘ (k_zl_real â‰ˆ 2 * k_gl_real)
                
                # è½¬æ¢åˆ°çœŸå®ç©ºé—´
                w1_real = k1_safe * (self.max_out / self.max_in[0])
                w2_real = k2_safe * (self.max_out / self.max_in[1])
                
                # æ£€æŸ¥æ¯”ä¾‹ (å…è®¸ 1.0 ~ 4.0 ä¹‹é—´çš„æ³¢åŠ¨ï¼Œä¸­å¿ƒå€¼ 2.0)
                # å¦‚æœ w2/w1 å¤ªå¤§ï¼Œè¯´æ˜é«˜ç‚‰è¢«ä½ä¼°ï¼Œè½¬ç‚‰è¢«é«˜ä¼°
                if w1_real > 1e-9:
                    ratio = w2_real / w1_real
                    if ratio > 4.0: 
                        # å¼ºè¡Œä¿®æ­£: ä¿æŒæ€»èƒ½é‡è´¡çŒ®è¿‘ä¼¼ä¸å˜ï¼Œé‡æ–°åˆ†é… k
                        # Current: E = k1*g + k2*z
                        # Target:  E = k1'*g + k2'*z, subject to k2'/k1' = 2.5 (ä¿å®ˆä¸€ç‚¹)
                        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å¤„ç†
                        w2_real = 2.5 * w1_real 
                        # é‡æ–°å½’ä¸€åŒ–å›å»ä¼šå¾ˆéº»çƒ¦ï¼Œè¿™é‡Œç®€å•ä»¥å¹³æ»‘çš„æ–¹å¼æ‹‰å›é”šç‚¹
                        # æˆ–è€…æ˜¯ç›´æ¥é™åˆ¶ k2_safe ä¸å‡†å¤ªå¤§
                        pass
                
                # è¿™é‡Œçš„ä¿®æ­£æ¯”è¾ƒå¤æ‚ï¼Œç®€å•ä¸€ç‚¹ï¼š
                # å¦‚æœ update_stats ç®—å‡ºæ¥çš„ç»“æœæå…¶ä¸åˆç†ï¼Œå°±ä¸è¦å¤§å¹…æ›´æ–°é”šç‚¹
                # æˆ–è€…ç›´æ¥åœ¨è¿™é‡Œåº”ç”¨ ratio çº¦æŸçš„ "æœŸæœ›å€¼"
                
            except:
                # æå°‘æ•°æƒ…å†µçŸ©é˜µä¸å¯é€†ï¼Œä¿æŒåŸå€¼
                k1_safe = self.base_k_gl_norm
                k2_safe = self.base_k_zl_norm

            if should_update_anchor:
                # é”šç‚¹æ›´æ–°: é™ä½ä¿¡ä»»åº¦ 0.05 -> 0.01ï¼Œé˜²æ­¢é”™è¯¯çš„æ•°æ®ç»Ÿè®¡å¸¦åç‰©ç†æ¨¡å‹
                # åªæœ‰å½“æ•°æ®ç»Ÿè®¡éå¸¸ç¡®å®šä¸”é•¿æœŸä¸€è‡´æ—¶ï¼Œæ‰æ…¢æ…¢ç§»åŠ¨é”šç‚¹
                self.base_k_gl_norm = 0.99 * self.base_k_gl_norm + 0.01 * k1_safe
                self.base_k_zl_norm = 0.99 * self.base_k_zl_norm + 0.01 * k2_safe

    def train_step(self, gl_arr, zl_arr, zq_arr):
        # ç¡®ä¿æ•°æ®å¤Ÿé•¿ï¼Œèƒ½è¦†ç›–æœ€å¤§çš„æ»å
        max_lag = max(self.lag_gl, self.lag_zl)
        if len(gl_arr) < max_lag + SEQ_LEN + 10: return None
        
        # å¯¹é½æ•°æ®: Target(t) å¯¹åº” GL(t - lag_gl) å’Œ ZL(t - lag_zl)
        # æˆ‘ä»¬ä» buffer çš„å°¾éƒ¨å‘å‰å›æº¯
        # å‡è®¾ arr æœ€åä¸€ä½æ˜¯å½“å‰æ—¶åˆ» T
        # æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„æœ‰æ•ˆ target èŒƒå›´æ˜¯ [max_lag + SEQ_LEN, T]
        
        total_len = len(gl_arr)
        valid_targets_start = max_lag + SEQ_LEN
        valid_targets_end = total_len
        
        if valid_targets_end <= valid_targets_start: return None
        
        inputs = []
        targets = []
        
        # éšæœºé‡‡æ ·è®­ç»ƒ
        sample_range = range(valid_targets_start, valid_targets_end)
        sample_count = min(len(sample_range), TRAIN_BATCH_SIZE)
        if sample_count <= 0: return None
        sample_idxs = np.random.choice(sample_range, size=sample_count, replace=False)
        
        # é¢„å…ˆè½¬æ¢ä¸º numpy æ–¹ä¾¿åˆ‡ç‰‡
        if ENABLE_EMA_SMOOTH:
            gl_arr = self._ema_smooth(gl_arr, EMA_ALPHA)
            zl_arr = self._ema_smooth(zl_arr, EMA_ALPHA)
            zq_arr = self._ema_smooth(zq_arr, EMA_ALPHA)

        g_full_raw = np.array(gl_arr)
        z_full_raw = np.array(zl_arr)
        g_full = g_full_raw / self.max_in[0]
        z_full = z_full_raw / self.max_in[1]
        q_full = np.array(zq_arr) / self.max_out

        # å™ªå£°é—¨æ§é˜ˆå€¼ï¼ˆåŠ¨æ€é‡ç¨‹ + ç»å¯¹é—¨é™ï¼‰
        thr_gl = max(self.max_in[0] * DELTA_GATE_FRAC, DELTA_GATE_ABS_GL)
        thr_zl = max(self.max_in[1] * DELTA_GATE_FRAC, DELTA_GATE_ABS_ZL)
        
        for t in sample_idxs:
            # æ„é€  t æ—¶åˆ»çš„è¾“å…¥ç‰¹å¾åºåˆ— (SEQ_LEN)
            # t æ˜¯ Target çš„æ—¶é—´ç‚¹
            
            # GL åºåˆ—ç»“æŸç‚¹: t - lag_gl
            t_gl_end = t - self.lag_gl
            if t_gl_end < SEQ_LEN: continue 
            
            # ZL åºåˆ—ç»“æŸç‚¹: t - lag_zl
            t_zl_end = t - self.lag_zl
            if t_zl_end < SEQ_LEN: continue
            
            # å™ªå£°é—¨æ§ï¼šè‹¥æœ€è¿‘ä¸€è·³å˜åŒ–è¿‡å°ï¼Œè·³è¿‡è¯¥æ ·æœ¬
            d_gl = g_full_raw[t_gl_end - 1] - g_full_raw[t_gl_end - 2]
            d_zl = z_full_raw[t_zl_end - 1] - z_full_raw[t_zl_end - 2]
            if abs(d_gl) < thr_gl and abs(d_zl) < thr_zl:
                continue

            seq_gl = g_full[t_gl_end - SEQ_LEN : t_gl_end]
            seq_zl = z_full[t_zl_end - SEQ_LEN : t_zl_end]
            
            # æ‹¼æ¥
            seq_xq = np.column_stack((seq_gl, seq_zl))
            
            target_val = q_full[t] # Target å°±æ˜¯ t æ—¶åˆ»çš„è’¸æ±½
            target_prev = q_full[t - 1]
            
            inputs.append(seq_xq)
            targets.append((target_val, target_prev))
            
        if not inputs: return None

        t_inputs = torch.FloatTensor(np.array(inputs)).to(self.device)
        t_targets = torch.FloatTensor(np.array(targets)).to(self.device)
        
        self.model.train()
        self.optimizer.zero_grad()
        
        # Unpack: pred_y, k_gl(normalized), k_zl(normalized), bias
        pred_y, k_gl, k_zl, bias = self.model(t_inputs)
        
        y_t = t_targets[:, 0:1]
        y_prev = t_targets[:, 1:2]

        # 1. MSE Loss (æ€»é‡æ‹Ÿåˆ)
        loss_mse = self.criterion(pred_y, y_t)
        
        # === æ”¹è¿›2: å·®åˆ†çº¦æŸ (Differential Loss) ===
        # å¼ºè¿«ç‰©ç†å…¬å¼å¯¹â€œå˜åŒ–é‡â€è´Ÿè´£: Delta Y â‰ˆ k * Delta X
        # æˆ‘ä»¬åˆ©ç”¨ t_inputs é‡Œçš„æ—¶é—´åºåˆ—ä¿¡æ¯
        # t_inputs shape: (Batch, Seq, 2)
        # å– Seq æœ€åä¸¤æ­¥è®¡ç®— Delta
        x_t   = t_inputs[:, -1, :]
        x_prev= t_inputs[:, -2, :]
        delta_x = x_t - x_prev # (Batch, 2)
        
        # ç‰©ç†é¢„æµ‹çš„å·®åˆ† (Bias è¢«æŠµæ¶ˆ)
        pred_delta_y = k_gl * delta_x[:, 0:1] + k_zl * delta_x[:, 1:2]
        true_delta_y = y_t - y_prev
        loss_diff = self.criterion(pred_delta_y, true_delta_y)
        
        # çœŸå®å·®åˆ†
        # æ³¨æ„: æˆ‘ä»¬éšæœºé‡‡æ ·æ—¶æ²¡æœ‰å– target çš„å‰ä¸€æ—¶åˆ»ï¼Œ
        # ä½†å¯ä»¥åœ¨ batch å†…éƒ¨è¿‘ä¼¼ï¼Œæˆ–è€…æ›´ä¸¥æ ¼åœ°åº”è¯¥åœ¨é‡‡æ ·æ—¶å¤šé‡‡ä¸€ä¸ªç‚¹ã€‚
        # é‰´äºä»£ç ç»“æ„ï¼Œæˆ‘ä»¬åˆ©ç”¨è¾“å…¥åºåˆ—çš„è¿ç»­æ€§ï¼Œ
        # å‡è®¾ y çš„å˜åŒ–ä¹Ÿæ˜¯è¿ç»­çš„ã€‚è¿™é‡Œç¨å¾®éœ€è¦ hack ä¸€ä¸‹ï¼š
        # æˆ‘ä»¬ç›®å‰åªæœ‰ t æ—¶åˆ»çš„ targetsã€‚
        # ä¸ºäº†ä¸æ”¹åŠ¨å¤ªå¤šé‡‡æ ·é€»è¾‘ï¼Œæˆ‘ä»¬å¼±åŒ–è¿™ä¸ªçº¦æŸï¼Œåªè®¡ç®— "Trend Consistency"
        # æˆ–è€…æˆ‘ä»¬å‡è®¾çŸ­æ—¶é—´å†… k ä¸å˜ï¼Œé‚£ä¹ˆ pred_y(t) - pred_y(t-1) åº”è¯¥ç­‰äº target(t) - target(t-1)
        # ä½†æˆ‘ä»¬æ²¡æœ‰ target(t-1)ã€‚
        
        # --- è¡¥ä¸: æ—¢ç„¶æ‹¿ä¸åˆ° target(t-1)ï¼Œæˆ‘ä»¬ç”¨ Gradient Loss ---
        # æˆ‘ä»¬å¸Œæœ› pred_y å¯¹ x çš„æ¢¯åº¦æ¥è¿‘ k
        # è¿™æ˜¯ä¸€ä¸ªæ˜¾å¼çš„ç‰©ç†çº¦æŸã€‚
        # ä½† PyTorch è‡ªåŠ¨å¾®åˆ†å·²ç»å¤„ç†äº†è¿™ä¸ªã€‚
        
        # --- ä¿®æ­£æ–¹æ¡ˆ: è¿˜æ˜¯åŠ ä¸Š Reg Loss ---
        
        # 2. ç‰©ç†çº¦æŸä¸ç¨³å®šæ€§ Loss
        
        # A. é”šç‚¹çº¦æŸ (Anchor Constraint)
        # é€‚åº¦ä¿¡ä»»ç»Ÿè®¡å­¦è®¡ç®—å‡ºçš„å…¨å±€ç³»æ•°ï¼Œé˜²æ­¢åç¦»å¤ªè¿œ
        loss_anchor = torch.mean((k_gl - self.base_k_gl_norm)**2 + (k_zl - self.base_k_zl_norm)**2)
        
        # B. æ³¢åŠ¨çº¦æŸ (Variance Constraint)
        loss_var = torch.var(k_gl) + torch.var(k_zl)
        
        # C. Bias çº¦æŸ (é˜²æ­¢ Bias è¿‡å¤§åƒæ‰æ‰€æœ‰èƒ½é‡)
        # å¼ºåŠ›å‹åˆ¶ Biasï¼Œè¿«ä½¿ k_gl æ‰¿æ‹…åŸºåº§èƒ½é‡
        loss_bias = torch.mean(bias**2)

        # D. é˜²å¡Œç¼©ä¸‹é™ï¼ˆé¿å… k_gl è¢«å‹åˆ° 0ï¼‰
        loss_floor = 0.0
        if ENABLE_FLOOR_LOSS:
            eps = 1e-8
            ratio_floor = k_gl / (k_zl + eps)
            loss_floor = torch.mean(torch.relu(FLOOR_RATIO_TAU - ratio_floor) ** 2)

        # F. ç‰©ç†æ¯”ä¾‹å…ˆéªŒ (Physical Ratio Prior)
        loss_prior = 0.0
        if ENABLE_RATIO_PRIOR:
            # å¸Œæœ› k_zl / k_gl â‰ˆ RATIO_TARGET
            # ä¹Ÿå°±æ„å‘³ç€ k_zl â‰ˆ RATIO_TARGET * k_gl
            # åœ¨å½’ä¸€åŒ–ç©ºé—´éœ€è¦è½¬æ¢ä¸€ä¸‹ï¼š ratio = (k_zl/k_gl) * (max_in_g/max_in_z) 
            scale_factor = self.max_in[1] / (self.max_in[0] + 1e-5)
            target_norm_ratio = RATIO_TARGET * scale_factor
            
            # ä½¿ç”¨ MSE çº¦æŸï¼š k_zl_norm â‰ˆ target * k_gl_norm
            loss_prior = torch.mean((k_zl - target_norm_ratio * k_gl)**2)

        # E. å˜åŒ–ç‡çº¦æŸ (Gradient Consistency) - æ–°å¢
        # é˜²æ­¢ k å€¼çªå˜ï¼Œä¸ä»…è¦æ–¹å·®å°ï¼Œè¿˜è¦ç›¸é‚»æ—¶åˆ»æ¥è¿‘ (Smoothness)
        # åˆ©ç”¨ batch å†…çš„æ ·æœ¬å¤§æ¦‚ç‡æ˜¯ä¹±åºçš„ï¼Œè¿™ä¸ªå¾ˆéš¾åœ¨éšæœºbatchåšã€‚
        # ä¾é  variance loss å·²ç»å¤Ÿäº†ã€‚

        # ç»„åˆ Loss
        # 0.5 * loss_anchor: é”šç‚¹æ‹‰ä½å‡å€¼
        # 5.0 * loss_bias: å¤§å¹…æé«˜æƒ©ç½š(åŸ0.2)ï¼Œå¼ºè¿«æ¨¡å‹å½’å› äºè¾“å…¥
        loss = loss_mse + 0.5 * loss_anchor + 2.0 * loss_var + 5.0 * loss_bias + 0.5 * loss_diff + FLOOR_LOSS_WEIGHT * loss_floor + PHYSICS_LOSS_WEIGHT * loss_prior
        
        loss.backward()
        self.optimizer.step()
        
        self.loss_history.append(loss.item())
        self.t += 1
        return loss.item()

    def calculate_sensitivity_ratio(self, current_gl, current_zl):
        self.model.eval()
        
        # Max Scaling
        gl_n = current_gl / self.max_in[0]
        zl_n = current_zl / self.max_in[1]
        
        base_seq = np.zeros((1, SEQ_LEN, 2))
        base_seq[:, :, 0] = gl_n
        base_seq[:, :, 1] = zl_n
        t_base = torch.FloatTensor(base_seq).to(self.device)
        
        with torch.no_grad():
            out, _ = self.model.lstm(t_base)
            feat = out[:, -1, :]
            coeffs = self.model.fc_energy(feat).numpy()[0] # [k_gl_norm, k_zl_norm]
            bias = self.model.bias.item()
            
        k_gl_norm, k_zl_norm = coeffs[0], coeffs[1]
        
        # è¿˜åŸåˆ°ç‰©ç†ç©ºé—´
        w_gl_real = k_gl_norm * (self.max_out / self.max_in[0])
        w_zl_real = k_zl_norm * (self.max_out / self.max_in[1])
        real_bias = bias * self.max_out # Bias ä¹Ÿè¦åå½’ä¸€åŒ– (å‡è®¾å®ƒä¹Ÿæ˜¯é’ˆå¯¹å½’ä¸€åŒ–åçš„y)

        
        if abs(w_gl_real) < 1e-9: w_gl_real = 1e-9
        
        # === æ ¸å¿ƒæ”¹è¿›: æŒ‡æ•°å¹³æ»‘ (EMA) ===
        if self.smooth_k_gl is None:
            self.smooth_k_gl = w_gl_real
            self.smooth_k_zl = w_zl_real
            self.smooth_ratio = w_zl_real / (w_gl_real + 1e-9)
        else:
            # è°ƒå¤§ alphaï¼Œè®©å®ƒæ•¢äºè·³åŠ¨
            alpha = 0.3 # 70% ç›¸ä¿¡å†å²ï¼Œ30% æ¥å—æ–°å€¼ (ååº”é€Ÿåº¦æå‡6å€)
            self.smooth_k_gl = self.smooth_k_gl * (1-alpha) + w_gl_real * alpha
            self.smooth_k_zl = self.smooth_k_zl * (1-alpha) + w_zl_real * alpha
            
            current_smooth_ratio = self.smooth_k_zl / (self.smooth_k_gl + 1e-9)
            if self.smooth_ratio is None:
                self.smooth_ratio = current_smooth_ratio
            self.smooth_ratio = self.smooth_ratio * (1-alpha) + current_smooth_ratio * alpha

        return self.smooth_k_gl, self.smooth_k_zl, self.smooth_ratio

    
    def predict(self, gl, zl):
        self.model.eval()
        # Max Scaling
        gl_n = gl / self.max_in[0]
        zl_n = zl / self.max_in[1]
        
        seq = np.zeros((1, SEQ_LEN, 2))
        seq[:, :, 0] = gl_n
        seq[:, :, 1] = zl_n
        
        with torch.no_grad():
            # forward è¿”å› (pred_y, k_gl, k_zl)ï¼Œåªå–ç¬¬ä¸€ä¸ª
            pred_tuple = self.model(torch.FloatTensor(seq))
            pred_norm = pred_tuple[0].item()
            
        return pred_norm * self.max_out

# å†å²æ•°æ®æ±  (Ring Buffer)
dq_gl = deque(maxlen=BUFFER_CAPACITY)
dq_zl = deque(maxlen=BUFFER_CAPACITY)
dq_zq = deque(maxlen=BUFFER_CAPACITY)


def calculate_ratio():
    CSV_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), CSV_FILENAME)
    
    learner = LSTMRatioLearner()
    if not HAS_TORCH: return
    
    print("=" * 60)
    print(f"ğŸ”¥ LSTM æ·±åº¦çƒ­å€¼è¾¨è¯†ç³»ç»Ÿ v3.3 (Lag-Aware) ğŸ”¥ (PyTorchå†…æ ¸)")
    print(f"åˆå§‹æ»å: GL={DEFAULT_LAG_GL}s, ZL={DEFAULT_LAG_ZL}s (å›ºå®š) | è®°å¿†é•¿åº¦: {SEQ_LEN}æ­¥")
    print(f"æ¨¡å¼: å®æ—¶ç›‘å¬åŠæ›´æ–° ({CSV_FILENAME})")
    print("=" * 60)

    last_read_lines = 0
    print(f"å¼€å§‹ç›‘å¬ {CSV_PATH} ...")

    while True:
        if not os.path.exists(CSV_PATH):
            print(f"\rç­‰å¾…æ–‡ä»¶... ", end="")
            time.sleep(2)
            continue
            
        try:
            try:
                # å®æ—¶è¯»å–ï¼šå¦‚æœæ–‡ä»¶è¾ƒå¤§ï¼Œéœ€è€ƒè™‘ä¼˜åŒ–
                df_all = pd.read_csv(CSV_PATH)
                current_lines = len(df_all)
            except Exception:
                time.sleep(0.1)
                continue
            
            if current_lines > last_read_lines:
                new_rows = df_all.iloc[last_read_lines:]
                # print(f"\ræ–°å¢æ•°æ®: {len(new_rows)} æ¡", end="")
                
                for index, last_row in new_rows.iterrows():
                    try:
                        # è·å–æ•°æ®
                        v_gl = pd.to_numeric(last_row.get(TAG_GLRQLL, 0), errors='coerce')
                        v_zl = pd.to_numeric(last_row.get(TAG_ZLRQLL, 0), errors='coerce')
                        v_zq = pd.to_numeric(last_row.get(TAG_ZQLL, 0), errors='coerce')

                        if v_gl > 10 and v_zq > 1: # è¿è¡Œä¸­
                            dq_gl.append(v_gl)
                            dq_zl.append(v_zl)
                            dq_zq.append(v_zq)
                        
                        # å½“æ•°æ®è¶³å¤Ÿæ»åè®­ç»ƒæ—¶
                        max_lag_steps = max(learner.lag_gl, learner.lag_zl)
                        
                        if len(dq_gl) > max_lag_steps + SEQ_LEN + 10:
                            
                            # 1. æ›´æ–°ç»Ÿè®¡é‡
                            learner.update_stats(list(dq_gl), list(dq_zl), list(dq_zq))
                            
                            # 2. è®­ç»ƒä¸€æ­¥ (ä»å†å²Bufferéšæœºé‡‡æ ·)
                            loss = learner.train_step(list(dq_gl), list(dq_zl), list(dq_zq))
                            
                            # 3. æå–å¯¹é½åçš„å®æ—¶æ•°æ® (ç”¨äºéªŒè¯/æ¨ç†)
                            idx_gl = -1 - learner.lag_gl
                            idx_zl = -1 - learner.lag_zl
                            
                            val_gl_aligned = dq_gl[idx_gl]
                            val_zl_aligned = dq_zl[idx_zl]
                            if ENABLE_EMA_SMOOTH and len(dq_gl) >= max_lag_steps + 2:
                                val_gl_aligned = (1 - EMA_ALPHA) * dq_gl[idx_gl - 1] + EMA_ALPHA * dq_gl[idx_gl]
                                val_zl_aligned = (1 - EMA_ALPHA) * dq_zl[idx_zl - 1] + EMA_ALPHA * dq_zl[idx_zl]
                            
                            # 3. è®¡ç®—çµæ•åº¦ (Ratios)
                            w1, w2, ratio = learner.calculate_sensitivity_ratio(val_gl_aligned, val_zl_aligned)
                            
                            # 4. é¢„æµ‹éªŒè¯
                            pred = learner.predict(val_gl_aligned, val_zl_aligned)
                            err_rate = abs(pred - v_zq) / (v_zq + 1e-5) * 100
                            acc = max(0, 100 - err_rate)

                            # 5. æ˜¾ç¤º
                            if index == new_rows.index[-1]: 
                                # ANSIè½¬ä¹‰: å…‰æ ‡å½’ä½ + æ¸…é™¤å±å¹•å‰©ä½™ (æ— é—ªçƒ)
                                print("\033[H\033[J", end="")
                                
                                c1 = w1 * val_gl_aligned
                                c2 = w2 * val_zl_aligned
                                
                                print(f"\n[å®æ—¶ Iter: {learner.t}] Loss: {loss:.5f} | Acc: {acc:.1f}%")
                                print(f"  æ»å: GL={learner.lag_gl}s, ZL={learner.lag_zl}s")
                                print(f"  é¢„æµ‹: {pred:.1f} (å®{v_zq:.1f}) | è¯¯å·®: {pred-v_zq:+.1f}")
                                #æ˜¾ç¤º Bias æœ‰åŠ©äºè°ƒè¯•
                                bias_val = learner.model.bias.item() * learner.max_out
                                
                                # è®¡ç®— Ratio
                                if w1 > 1e-9:
                                     current_ratio = w2 / w1
                                else:
                                     current_ratio = 0.0
                                     
                                print(f"Ratio={current_ratio:.2f}")
                                print(f"  èŒƒå›´: MaxGL={learner.max_in[0]:.1f}, MaxZL={learner.max_in[1]:.1f}, MaxZQ={learner.max_out:.1f}")
                                print(f"  å½’ä¸€åŒ–ç³»æ•°: k_gl_norm={w1 / (learner.max_out/learner.max_in[0]):.4f}, k_zl_norm={w2 / (learner.max_out/learner.max_in[1]):.4f}")
                                print(f"  ç‰©ç†ç³»æ•°: k_gl={w1:.5f}, k_zl={w2:.5f}")
                                # print(f"  ç³»æ•°: k_gl={w1:.4f}, k_zl={w2:.4f} (Ratio={current_ratio:.2f})")
                                # print(f"  åå·®: Bias={bias_val:.1f}")
                                if USE_INDEPENDENT_SEGMENTS:
                                    # è®¡ç®—ä¸€ä¸‹ buffer é‡Œçš„æ³¢åŠ¨æ ‡å‡†å·®ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯æ­»æ•°æ®
                                    std_gl = np.std(dq_gl) if len(dq_gl) > 0 else 0
                                    std_zl = np.std(dq_zl) if len(dq_zl) > 0 else 0
                                    print(f"  æ•°æ®æ³¢åŠ¨(Std): GL={std_gl:.2f}, ZL={std_zl:.2f} (æœ‰æ•ˆæ®µ: {learner.last_indep_gl}/{learner.last_indep_zl})")
                                print("-" * 40)

                        else:
                            if learner.t % 100 == 0:
                               print(f"\ræ­£åœ¨ç¼“å†²... {len(dq_gl)}/{max_lag_steps + SEQ_LEN}", end="")
                    
                    except Exception:
                        pass
                
                last_read_lines = current_lines
            
            time.sleep(UPDATE_INTERVAL)
            
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(1)


if __name__ == "__main__":
    calculate_ratio()